{"cells":[{"cell_type":"markdown","metadata":{"id":"tb9EIYAir7vM"},"source":["# <font color='orange'><b>LORA를 활용한 LLM fine-tuning with Ko-Platypus🥮</b></font>\n","\n","<img src = \"https://drive.google.com/uc?id=1OjplFXkW2KzgFZeGwsV1wAWR6oeIiIv2\" height=512, width=512>  \n","[Ko-platypus](https://github.com/Marker-Inc-Korea/KO-Platypus)  "]},{"cell_type":"markdown","metadata":{"id":"lqCodfTcS_RC"},"source":["-------\n","## 1. Colab 환경설정👨‍💻\n","- Colab Pro\n","- GPU A100 활용\n","\n","\n","## 2. Ko-Platypus 실습내용📖\n","> 이번 실습코드에서 다뤄질 내용들은 다음과 같이 6개로 구성되어 있습니다!  \n","  \n","- Code 환경설정\n","- Hyperparameters in LLM\n","- <font color='orange'><b>LoRA🔥</b></font>\n","- Instruction input-output dataset\n","- <font color='orange'><b>LLM fine-tuning🤗</b></font> (with ko-openorca dataset🐳)\n","- <font color='yellow'><b>⭐Model Upload⭐</b></font>\n","\n","  \n","## 3. 실습에 이용되는 데이터셋과 모델📖\n","- ✅데이터: [kyujinpy/Open-platypus-Commercial](https://huggingface.co/datasets/kyujinpy/Open-platypus-Commercial)  \n","- ✅모델: [upstage/SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)    \n","  \n","   \n","## 4. 참고자료\n","- ⭐[Platypus](https://github.com/arielnlee/Platypus)  \n","- ⭐[garage-bAInd/Open-Platypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)  \n","- ⭐[KO-Platypus2-7B-ex🥮](https://huggingface.co/kyujinpy/KO-Platypus2-7B-ex)\n","- [HuggingFace](https://huggingface.co/)  \n","- [KO-LLM LeaderBoard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)  \n","- [EN-LLM LeaderBoard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n"]},{"cell_type":"markdown","metadata":{"id":"vb3N62XRvnud"},"source":["--------\n","# 👨‍💻 HuggingFace 로그인\n","> 본격적인 시작에 앞서서 Huggingface에 회원가입을 진행해볼까요?\n","\n","1. 아래의 링크로 접속하여 회원가입을 진행합니다.\n","  - 🤗https://huggingface.co/\n","\n","2. 자신의 Access token을 생성합니다.\n","  - ✅(메인 화면) 우측 상단의 자신의 프로필을 클릭한 후, `setting`을 누른다.\n","  - ✅(프로필 화면) 왼쪽 카테고리에서 `Access Token`을 누른다.\n","  - ✅오른쪽에서 `New token` 눌러서 `Role: read`로 설정한 후 자신의 token을 생성한다.\n","  - ✅다시 한번, `New token` 눌러서 `Role: write`로 설정한 후 자신의 token을 생성한다.\n","\n","3. 코드에서 Huggingface login 진행을 합니다.\n","  - ⭐`Role: read`로 생성된 token을 복사 한 후, 아래의 `[...read_token...]` 부분에 붙여넣는다.\n","```python\n","!huggingface-cli login --token [...read_token...]\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JrDD6zSIvaAl"},"outputs":[],"source":["###########################################\n","# 0. Huggingface login\n","\n","# READ token\n","!huggingface-cli login --token [...read_token...] # your code"]},{"cell_type":"markdown","metadata":{"id":"2SXk0puNOpQ7"},"source":["--------\n","# 😎 Colab 환경설정\n","> 본격적인 시작에 앞서서 colab의 환경설정을 진행해볼까요?\n","\n","🔎 LLM을 훈련시키기 위해서 아래의 모듈을 필수적으로 설치해야합니다!\n","- 🤗transformers: LLM fine-tuning 및 전반적인 기능\n","- 🤗bitsandbytes: bit 단위로 모델을 불러올 수 있는 기능\n","- 🤗accelerate: GPU 가속\n","- 🔥loralib: LoRA 세팅\n","- 🤗datasets: 데이터셋 활용\n","- 💎peft: Parameter Efficient Fine Tuning 적용\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRx1ZaJOrtOz"},"outputs":[],"source":["###########################################\n","# 1-1. 구글 드라이브 마운트\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tS43LmbkUBCG"},"outputs":[],"source":["###########################################\n","# 1-2. 프로젝트 폴더 만들기\n","\n","import os\n","os.makedirs('/content/drive/MyDrive/FastCampus-LLM', exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L92Xa0AwH1cz"},"outputs":[],"source":["%cd /content/drive/MyDrive/FastCampus-LLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z72x_0sBUkNo"},"outputs":[],"source":["##########################################\n","# 1-3. 모듈 다운\n","\n","!pip install bitsandbytes==0.41.1\n","!pip install accelerate==0.21.0\n","!pip install appdirs\n","!pip install loralib\n","!pip install black black[jupyter]\n","!pip install datasets\n","!pip install fire\n","!pip install git+https://github.com/huggingface/peft\n","!pip install transformers==4.34.1\n","!pip install sentencepiece sentence_transformers\n","!pip install scipy numpy scikit-learn pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gxwVD1IVVmez"},"outputs":[],"source":["###########################################\n","# 1-4. 설치된 모듈 리스트 확인하기\n","\n","!pip list"]},{"cell_type":"markdown","metadata":{"id":"GtHstE0tWJrm"},"source":["--------\n","# 😎 모듈 불러오기\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6-72g-XWINm"},"outputs":[],"source":["###########################################\n","# 2-1. 모듈 불러오기\n","\n","import os\n","import os.path as osp\n","import sys\n","import fire\n","import json\n","from typing import List, Union\n","\n","import torch\n","from torch.nn import functional as F\n","\n","import transformers\n","from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n","from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n","from transformers import LlamaForCausalLM, LlamaTokenizer\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","from datasets import load_dataset\n","\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_int8_training,\n","    set_peft_model_state_dict\n",")\n","from peft import PeftModel"]},{"cell_type":"markdown","metadata":{"id":"upTfYsRNg7Dj"},"source":["--------\n","# 🤗 Base Model (LLM)\n","- 우리가 해당 실습코드에서 이용할 base LLM 모델 아래와 같습니다!\n","  - ✅모델: [upstage/SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0)    \n","\n","## ✔️ Model Access 확인\n","> ✅모델을 이용하기 전에 access가 있는지 꼭 확인하세요!  \n","\n","<img src = \"https://drive.google.com/uc?id=1oACzT9OwEAundjhEOBdFI7-nV99OrIm8\">  \n","- 위의 사진과 같은 <font color='#6069B9'><b>**✔️access**</b></font> 버튼이 있다면 꼭 눌러서 승인을 해야합니다!\n","- 예를 들어 meta-llama의 모델을 경우, 승인을 받아야 이용이 가능하다는 점도 참고해주세요!  \n","  - 🤗해당 모델이 아닌 다른 모델을 base LLM으로 활용하셔도 됩니다!\n","  \n","#### 😲 Error Note\n","> Colab에서 모델을 다운 받다가 아래와 같은 에러를 만났다면 한번 다음과 같이 해결해보세요!\n","  \n","```python\n","ImportError: Using `load_in_8bit=True` requires Accelerate: `pip install\n","accelerate` and the latest version of bitsandbytes `pip install -i https://\n","test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes`  \n","```\n","- **✅Colab의 런타임 유형이 A100인지 확인.**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nfiPHh5Sh0kp"},"outputs":[],"source":["#@title 🤗 Base model 선택하기\n","device = 'auto' #@param {type: \"string\"}\n","base_LLM_model = 'upstage/SOLAR-10.7B-v1.0' #@param {type: \"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIjvCtRJghoP"},"outputs":[],"source":["###########################################\n","# 3-1. 모델 다운로드 (~30분)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_LLM_model,\n","    load_in_8bit=True, # LoRA\n","    #load_in_4bit=True, # Quantization Load\n","    torch_dtype=torch.float16,\n","    device_map=device)\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_LLM_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1_K5QbnzxUP"},"outputs":[],"source":["###########################################\n","# 3-2. BOS, EOS, PAD 토큰 확인\n","\n","# Check special token\n","bos = tokenizer.bos_token_id # 문장 시작 토큰\n","eos = tokenizer.eos_token_id # 문장 끝 토큰\n","pad = tokenizer.pad_token_id # 문장 패딩 토큰\n","tokenizer.padding_side = \"right\" # 패딩 오른쪽\n","\n","print(\"BOS token:\", bos) # 1\n","print(\"EOS token:\", eos) # 2\n","print(\"PAD token:\", pad) # None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WOBun1kv0m_8"},"outputs":[],"source":["if (pad == None) or (pad == eos):\n","    tokenizer.pad_token_id = 0  # 만약 패딩값이 없거나 eos값과 같다면,\n","print(\"length of tokenizer:\",len(tokenizer)) # 32000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3ZSeYV0xf9Q"},"outputs":[],"source":["print(model)\n","print(type(model)) # 모델의 타입 확인"]},{"cell_type":"markdown","metadata":{"id":"nmZIeJR3HCwN"},"source":["--------\n","# 💡Hyperparameters setting\n","> 🔎 LLM fine-tuning에서 중요한 <font color='green'><b>hyperpamaeter</b></font>에 대해서 살펴볼까요?\n","\n","1. **📖cutoff_len**\n","> 모델에 들어갈 **sequence의 최대 길이를 설정**하는 parameter 입니다!\n","\n","2. **📖warmup_steps**\n","> warmup_steps은 천천히 learning rate를 올려서, 학습의 불안정성을 방지해줄 수 있습니다!\n","  \n","  - **Example)** learning rate (LR)을 1e-2로 설정했다면, LR을 처음에 초깃값보다 낮게 시작하여 100 step에 걸쳐서 LR를 1e-2로 올린다.\n","  \n","3. **📖Optimizer**\n","> 최근에는 Adam보다 AdamW를 더 많이 이용하고 있어요!  \n","> AdamW는 Adam에 비해서 더 <font color='#ea7a7a'><b>general</b></font>하게 성능을 만들어주는 장점이 있습니다!\n","  \n","  - Adam vs <font color='#ea7a7a'><b>**AdamW**</b></font>\n","\n","4. **📖lr_scheduler**\n","> lr_scheduler는 learning rate를 훈련하는 동안 조절해주는 옵션이예요!  \n","> 아래의 목록에 있는 총 3가지 함수 옵션이 가장 대표적으로 사용됩니다!\n","  \n","  - constant\n","  - linear\n","  - <font color='#ea7a7a'><b>consine</b></font>\n","\n","5. **📖Others**\n","> 😊그 외에 중요한 hyperparameter 옵션들을 모아서 정리해봤어요!\n","\n","  - **⭐Learning rate**\n","  - Batch size\n","  - weight_decay\n","  - max grad norm: gradient vector 크기 조절 (gradient clipping)"]},{"cell_type":"markdown","metadata":{"id":"zxrjlM6AdhJq"},"source":["## 👨‍💻 LoRA hyperparameters\n","> 🔎 PEFT의 대표적인 방법론인 <font color='orange'><b>LoRA🔥</b></font>의 <font color='green'><b>hyperpamaeter</b></font>에 대해서 살펴봅시다!\n","\n","<img src = \"https://drive.google.com/uc?id=1ZoFUOPAyfWx80qk9Oa6b8oY0h1bJRoE0\" width=512>\n","\n","  -  <font color='orange'><b>**Lora_r**</b></font>: LoRA adapter의 차원을 결정하는 파라미터\n","  - Lora_alpha: LoRA adapter의 scaling 값을 결정하는 파라미터\n","  - Lora_dropout: LoRA adapter의 dropout 파라미터\n","  -  <font color='orange'><b>**Lora_target_modules**</b></font>: LoRA adapter를 적용할 layers\n","  ```\n","Lora_target_modules 종류:\n","[\"embed_tokens\", \"lm_head\", \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", ...] # in llama2\n","  ```\n","> 🔎 LoRA가 어떻게 적용되는지 코드로 살펴볼까요?\n","  \n","  ```python\n","class Linear(nn.Linear, LoRALayer):\n","  # LoRA implemented in a dense layer\n","  def __init__(\n","          self,\n","          in_features: int,\n","          out_features: int,\n","          r: int = 0,\n","          lora_alpha: int = 1,\n","          lora_dropout: float = 0.,\n","          fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n","          merge_weights: bool = True,\n","          **kwargs\n","      ):\n","          nn.Linear.__init__(self, in_features, out_features, **kwargs)\n","          LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n","                              merge_weights=merge_weights)\n","          self.fan_in_fan_out = fan_in_fan_out\n","          # 실제 훈련할 파라미터\n","          if r > 0:\n","              self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n","              self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n","              self.scaling = self.lora_alpha / self.r\n","              # Freezing the pre-trained weight matrix\n","              self.weight.requires_grad = False\n","          self.reset_parameters()\n","          if fan_in_fan_out:\n","            self.weight.data = self.weight.data.transpose(0, 1)\n","  ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SphoNRsqHDNS"},"outputs":[],"source":["###########################################\n","# 5-1. 하이퍼 파라미터\n","\n","# 데이터셋과 훈련 횟수와 관련된 하이퍼 파라미터\n","batch_size = 16\n","num_epochs = 1\n","micro_batch = 1\n","gradient_accumulation_steps = batch_size // micro_batch\n","\n","# 훈련 방법에 대한 하이퍼 파라미터\n","cutoff_len = 4096\n","lr_scheduler = 'cosine'\n","warmup_ratio = 0.06 # warmup_steps = 100\n","learning_rate = 4e-4\n","optimizer = 'adamw_torch'\n","weight_decay = 0.01\n","max_grad_norm = 1.0\n","\n","# LoRA config\n","lora_r = 16\n","lora_alpha = 16\n","lora_dropout = 0.05\n","lora_target_modules = [\"gate_proj\", \"down_proj\", \"up_proj\"]\n","\n","# Tokenizer에서 나오는 input값 설정 옵션\n","train_on_inputs = False\n","add_eos_token = False\n","\n","# Others\n","resume_from_checkpoint = False # !! 만약 모델을 이어서 훈련하고 싶다면, './custom_LLM/checkpoint-[xxx]'와 같이 파일 경로를 입력해야 합니다!\n","output_dir = './custom_LLM'"]},{"cell_type":"markdown","metadata":{"id":"PNDw8EcnDgy0"},"source":["--------\n","# 🤗 Dataset Loading and formatting\n","> 👨‍💻 LLM fine-tuning을 위한 데이터셋을 불러오고 <font color='green'><b>instruction input-output</b></font>형태로 만들어 봅시다!\n","\n","- 우리가 해당 실습코드에서 이용할 데이터셋은 아래와 같습니다!\n","  - ✅데이터: [kyujinpy/Open-platypus-Commercial](https://huggingface.co/datasets/kyujinpy/Open-platypus-Commercial)    "]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"uc6eJ15pD3oq"},"outputs":[],"source":["#@title 🤗 Choose Dataset\n","dataset = 'kyujinpy/Open-platypus-Commercial' #@param {type: \"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7Ywoq65DevA"},"outputs":[],"source":["###########################################\n","# 5-1. 데이터셋 다운로드\n","\n","data = load_dataset(dataset)\n","print(data['train']) # 개수: 19079"]},{"cell_type":"markdown","metadata":{"id":"SrsYtTRuof_W"},"source":["## 🤗 Instruction input-output template\n","> 우리가 활용한 데이터는 아래와 같은 형식을 갖고 있습니다!\n","   \n","- ✅Input이 있는 example:  \n","  \n","```\n","Input:\n","당신은 사람들이 정보를 찾을 수 있도록 도와주는 인공지능 비서입니다.\n","사용자가 질문을 합니다. 당신의 임무는 가능한 한 충실하게 대답하는 것입니다.\n","답변하는 동안 단계별로 생각하고 답변을 정당화하세요.  \n","  \n","Instruction:\n","질문입니다: 시속 60km의 속도로 달리는 기차가 9초 만에 기둥을 통과합니다. 기차의 길이는 얼마입니까?\n","옵션: (A) 287 (B) 699 (C) 677 (D) 168 (E) 150 먼저 생각해 봅시다. 무작위로 추론해 봅시다:\n","  \n","Output:\n","기차의 길이를 구하려면 기차가 기둥을 통과하는 동안 이동하는 거리를 결정해야 합니다.\n","1단계: 기차의 속도를 초당 미터로 변환합니다. 1km = 1000미터, 1시간 = 3600초, 따라서 60km/hr = (60 * 1000) / 3600 = 16.67m/s(대략).\n","2단계: 기차가 9초 동안 이동한 거리를 계산합니다. 거리 = 속도 × 시간 거리 = 16.67m/s * 9초 = 150미터.\n","3단계: 계산된 거리를 주어진 옵션과 비교합니다.\n","계산된 거리(150미터)에 가장 가까운 옵션은 옵션 (E) 150입니다.\n","따라서 열차의 길이는 약 150미터입니다.\n","```\n","\n","  \n","- ✅Input이 없는 example:\n","\n","```\n","Instruction:\n","질문이 주어집니다: 다음 기사에 대한 객관식 문제를 작성하세요: 기사: 겨울은 무슨 일이 일어날지 알기 어렵고 사고가 너무 쉽게 일어나기 때문에 위험합니다.\n","안개가 언덕 꼭대기에서 여러분을 기다리고 있을 수 있습니다. 녹는 눈 밑에 얼음이 숨어 있다가 운전자를 도로 밖으로 내보내려고 기다리고 있을 수도 있습니다.\n","마주 오는 차가 갑자기 도로를 가로질러 미끄러질 수도 있습니다. 빙판길 운전의 첫 번째 규칙은 부드럽게 운전하는 것입니다.\n","갑작스러운 움직임은 자동차를 제어하기 매우 어렵게 만들 수 있습니다. 따라서 차를 출발하거나 정지할 때마다, 속도를 높이거나 낮출 때마다 가능한 한 부드럽고 천천히 운전해야 합니다.\n","옆 좌석에 뜨거운 커피 한 잔을 가득 들고 운전한다고 가정해 보겠습니다. 커피를 엎지르지 않도록 운전하세요.\n","두 번째 규칙은 무슨 일이 일어날지 주의를 기울이는 것입니다. 얼음이 많을수록 도로 아래쪽을 더 잘 살펴야 합니다.\n","차를 부드럽게 정지하는 데 걸리는 시간을 테스트하세요. 생각보다 더 빨리 운전할 수 있다는 점을 기억하세요.\n","일반적으로 도로가 젖어 있을 때는 평소 정지 거리의 두 배, 눈이 쌓여 있을 때는 이 거리의 세 배, 빙판길에서는 그보다 더 많은 거리를 확보하세요.\n","항상 차량을 통제하려고 노력하면 문제가 발생하지 않습니다. 정답은\n","\n","Output:\n","빙판길 운전 시 가장 중요한 규칙은 무엇인가요?\n","A) 위험 지역을 더 빨리 통과하기 위해 속도를 높입니다.\n","B) 잠재적 위험을 피하기 위한 갑작스러운 움직임\n","C) 부드럽고 부드럽게 운전하세요\n","D) 바로 앞 도로에만 집중하기\n","```"]},{"cell_type":"markdown","metadata":{"id":"AbgxRlpzz9ms"},"source":["> 🔎 Alpaca template를 활용하여서 우리가 이용하고자 하는 데이터셋을 instruction input-output 형식으로 재정의해볼까요?\n","- [Alpaca github](https://github.com/tatsu-lab/stanford_alpaca)   \n","  \n","#### ✅영어 버전 template\n","```\n","{\n","    \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n","    \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n","    \"response_split\": \"### Response:\"    \n","}\n","\n","```\n","   \n","#### ✅한국어 버전 template\n","```\n","{\n","    \"prompt_input\": \"아래는 작업을 설명하는 지침과 추가 입력을 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\\n\\n### 지침:\\n{instruction}\\n\\n### 입력:\\n{input}\\n\\n### 답변:\\n\",\n","    \"prompt_no_input\" : \"아래는 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\\n\\n### 지침:\\n{instruction}\\n\\n### 답변:\\n\",\n","    \"response_split\": \"### 답변:\"\n","}\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NK6EyQexOTBj"},"outputs":[],"source":["###########################################\n","# 5-2. Instruction tuning을 위한 template 작성.\n","\n","instruct_template = {\n","    \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n","    \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n","    \"response_split\": \"### Response:\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8f63aofNTWP"},"outputs":[],"source":["###########################################\n","# 5-3. 데이터셋 불러오는 클래스\n","\n","class Prompter(object):\n","\n","    def __init__(self, verbose: bool = False):\n","        self.template = instruct_template\n","\n","    def generate_prompt(\n","        self,\n","        instruction: str,\n","        input: Union[None, str] = None,\n","        label: Union[None, str] = None,\n","    ) -> str:\n","\n","        if input: # input text가 있다면\n","            res = self.template[\"prompt_input\"].format(\n","                instruction=instruction, input=input\n","            )\n","        else:\n","            res = self.template[\"prompt_no_input\"].format(\n","                instruction=instruction\n","            )\n","\n","        if label:\n","            res = f\"{res}{label}\"\n","\n","        return res\n","\n","    def get_response(self, output: str) -> str:\n","        return output.split(self.template[\"response_split\"])[1].strip()\n","\n","prompter = Prompter()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWdjgMS6Gi6x"},"outputs":[],"source":["###########################################\n","# 5-4. Token generation 함수\n","\n","def tokenize(prompt, add_eos_token=True):\n","    result = tokenizer(\n","        prompt,\n","        truncation=True,\n","        max_length=cutoff_len,\n","        padding=False,\n","        return_tensors=None,\n","    )\n","\n","    if (\n","        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n","        and len(result[\"input_ids\"]) < cutoff_len\n","        and add_eos_token\n","    ):\n","\n","        result[\"input_ids\"].append(tokenizer.eos_token_id)\n","        result[\"attention_mask\"].append(1)\n","\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","\n","    return result\n","\n","def generate_and_tokenize_prompt(data_point):\n","    full_prompt = prompter.generate_prompt(\n","        data_point[\"instruction\"],\n","        data_point[\"input\"],\n","        data_point[\"output\"])\n","\n","    tokenized_full_prompt = tokenize(full_prompt)\n","    if not train_on_inputs:\n","\n","        user_prompt = prompter.generate_prompt(\n","            data_point[\"instruction\"], data_point[\"input\"])\n","\n","        tokenized_user_prompt = tokenize(\n","            user_prompt, add_eos_token=add_eos_token)\n","\n","        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n","\n","        if add_eos_token:\n","            user_prompt_len -= 1\n","\n","        tokenized_full_prompt[\"labels\"] = [\n","            -100\n","        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n","            user_prompt_len:\n","        ]\n","    return tokenized_full_prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-ncCWzWQr4h"},"outputs":[],"source":["###########################################\n","# 5-5. 훈련 셋 만들기 (~3분)\n","\n","val_data = None\n","train_data = data[\"train\"].shuffle() # random\n","train_data = train_data.map(generate_and_tokenize_prompt)"]},{"cell_type":"markdown","metadata":{"id":"v8RcQ-wiSr1N"},"source":["#### ✅'prompt_input' example:\n","\n","```\n","아래는 작업을 설명하는 지침과 추가 입력을 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\n","\n","### 지침:\n","다음 질문에 답하세요: 제목: 내가 본 최악의 영화 리뷰: 이것은 내가 본 최악의 영화입니다... DVD를 구입하는 유일한 이유는 이 영화를 만든 12살짜리 꼬마가 저능아 제작자에게 팔아넘긴 진짜 감독의 코멘터리를 공개하기 위해서입니다. 가장 좋은 부분은 비디오 게임 클립을 이어 붙인 것을 볼 때였을 것입니다. 이 상품평은 부정적인가요?\n","답변:\n","\n","### 입력:\n","귀하는 지시를 매우 잘 따르는 인공지능 비서입니다. 최대한 많이 도와주세요.\n","\n","### 답변:\n","예, 이 제품 리뷰는 부정적입니다.</s>\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8NCt1X--S1Hv"},"source":["#### ✅'prompt_no_input' example:\n","\n","```\n","아래는 작업을 설명하는 지침입니다. 요청을 적절히 완료하는 답변을 작성해주세요.\n","\n","### 지침:\n","x+y = 10$, $2x+y = 13$이 주어졌을 때, $x^2-y^2$를 평가합니다.\n","\n","### 답변:\n","x^2-y^2$를 풀려면 $x$와 $y$의 값을 찾아야 합니다. 주어진 방정식을 사용하여 변수 중 하나를 제거하고 다른 변수를 풀 수 있습니다. 첫 번째 방정식을 두 번째 방정식에서 빼면 $2x+y - (x+y) = 13-10$이 나오며, 이는 $x = 3$으로 단순화됩니다. 그런 다음 첫 번째 방정식을 사용하여 $x = 3$을 대입하면 $3+y = 10$을 얻을 수 있으며, 이는 $y = 7$이라는 것을 의미합니다. 이제 $x = 3$과 $y = 7$을 얻었으므로 이를 $x^2-y^2$로 대입할 수 있습니다. 이렇게 하면 $3^2-7^2 = 9-49 = -40$이 됩니다.</s>\n","```"]},{"cell_type":"markdown","metadata":{"id":"LByHr8qpRk_8"},"source":["--------\n","# 🤗 Apply LoRA config\n","> ❗우리가 훈련에 이용할 base LLM에 LoRA를 적용해봅시다!❗"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6M37ePijRGws"},"outputs":[],"source":["###########################################\n","# 6-1. LoRA config 정의\n","\n","config = LoraConfig(\n","    r=lora_r,\n","    lora_alpha=lora_alpha,\n","    target_modules=lora_target_modules,\n","    lora_dropout=lora_dropout,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMB2WzT0SfGM"},"outputs":[],"source":["###########################################\n","# 6-2. Model with LoRA\n","\n","model = prepare_model_for_int8_training(model)\n","model = get_peft_model(model, config) # Applying LoRA"]},{"cell_type":"markdown","metadata":{"id":"zFEsYeMYTsUf"},"source":["--------\n","# ⭐ LLM fine-tuning\n","> 🥰 드디어 LLM fine-tuning을 위한 준비가 모두 끝났습니다! 한번 모델을 돌려볼까요?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tmSdPXhStNA"},"outputs":[],"source":["###########################################\n","# 7-1. 만약 이전에 돌렸던 모델을 가져온다면, 아래의 코드 실행\n","\n","if resume_from_checkpoint:\n","    checkpoint_name = os.path.join(\n","        resume_from_checkpoint, \"pytorch_model.bin\"\n","    )  # All checkpoint\n","\n","    if not os.path.exists(checkpoint_name):\n","        checkpoint_name = os.path.join(\n","            resume_from_checkpoint, \"adapter_model.bin\"\n","        )  # only LoRA model\n","        resume_from_checkpoint = (\n","            True\n","        ) # kyujin: I will use this checkpoint\n","\n","    if os.path.exists(checkpoint_name):\n","        print(f\"Restarting from {checkpoint_name}\")\n","        adapters_weights = torch.load(checkpoint_name)\n","        set_peft_model_state_dict(model, adapters_weights)\n","\n","    else:\n","        print(f\"Checkpoint {checkpoint_name} not found\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtZpAMd1TlMV"},"outputs":[],"source":["###########################################\n","# 7-2. Trainer class 정의\n","\n","trainer = transformers.Trainer(\n","        model=model,\n","        train_dataset=train_data,\n","        eval_dataset=val_data,\n","        args=transformers.TrainingArguments( # 훈련에 이용될 하이퍼파라미터\n","            per_device_train_batch_size = micro_batch,\n","            gradient_accumulation_steps = gradient_accumulation_steps,\n","            warmup_ratio=warmup_ratio,\n","            num_train_epochs=num_epochs,\n","            learning_rate=learning_rate,\n","            fp16=True,\n","            logging_steps=1,\n","            optim=\"adamw_torch\",\n","            evaluation_strategy=\"no\",\n","            save_strategy=\"steps\",\n","            max_grad_norm = max_grad_norm,\n","            save_steps = 30, # you can change!\n","            lr_scheduler_type=lr_scheduler,\n","            output_dir=output_dir,\n","            save_total_limit=2,\n","            load_best_model_at_end=False,\n","            ddp_find_unused_parameters=False,\n","            group_by_length = False\n","        ),\n","        data_collator=transformers.DataCollatorForSeq2Seq(\n","            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n","        ),\n","    )\n","\n","model.config.use_cache = False\n","model.print_trainable_parameters() # 훈련하는 파라미터의 % 체크\n","\n","if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n","    model = torch.compile(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DX8RW5YPUxWY"},"outputs":[],"source":["###########################################\n","# 7-3. Training (fine-tuning)\n","\n","## 훈련시간이 많이 소요됩니다! (~12h)\n","## 만약 중간에 colab이 끊긴다면, checkpoint를 이용해서 다시 훈련하세요!\n","### 만약 개인적인 GPU 자원이 있다면, 개인 서버나 컴퓨터에서 훈련시키는 것을 추천드립니다!\n","\n","torch.cuda.empty_cache()\n","trainer.train(resume_from_checkpoint=resume_from_checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ad0OAYkR0TtG"},"outputs":[],"source":["###########################################\n","# 7-4. 모델 저장\n","\n","model.save_pretrained(output_dir)\n","model_path = os.path.join(output_dir, \"pytorch_model.bin\")\n","torch.save({}, model_path)\n","tokenizer.save_pretrained(output_dir)"]},{"cell_type":"markdown","metadata":{"id":"8NNao1MNzUtB"},"source":["-------\n","# 🤗 LoRA Adapter merge\n","> 🎉 드디어 LLM fine-tuning이 끝났습니다! 여기까지 오시느라 정말 고생하셨습니다!! 🎉  \n","> 🔎 이제 훈련한 LoRA layer를 기존 base LLM에 붙여서 저장해볼까요!?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LRcka4btzTMO"},"outputs":[],"source":["###########################################\n","# 8-1. 훈련된 LoRA layer와 base LLM 병합(merge)\n","\n","torch.cuda.empty_cache()\n","\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    base_LLM_model,\n","    return_dict = True,\n","    torch_dtype=torch.float16,\n","    device_map=device)\n","\n","model = PeftModel.from_pretrained(base_model, output_dir, device)\n","model = model.merge_and_unload() # Merge!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qe6w9jvnzTFz"},"outputs":[],"source":["###########################################\n","# 8-2. 여러분의 custom LLM 모델 저장!\n","\n","final_save_folder = './custom_LLM_final'\n","\n","model.save_pretrained(final_save_folder)\n","tokenizer.save_pretrained(final_save_folder)"]},{"cell_type":"markdown","metadata":{"id":"n8ViOCSH_8Ha"},"source":["-------\n","# 🤗 HuggingFace에 모델 업로드\n","> ✔️마지막으로, Huggingface에 자신의 모델을 직접 업로드를 해봅시다!\n","\n","1. 코드에서 Huggingface login 진행합니다.\n","  - HuggingFace에 다시 접속하여, access token 카테고리 접속한다.\n","  - ⭐`Role: write`로 생성된 token을 복사 한 후, 아래의 `[...write_token...]` 부분에 붙여넣는다.\n","```\n","!huggingface-cli login --token [...write_token...]\n","```\n","  - ✅Huggingface에 접속하여, 모델을 업로드 할 repo를 생성한다.\n","    - ✅사전에 제공된`README.md` 파일을 이용하여 자신의 model card를 작성한다.\n","  - ✅아래의 코드를 이용하여 모델과 tokenizer를 업로드한다.\n","```\n","model = AutoModelForCausalLM.from_pretrained(\"./custom_LLM_final\")\n","model.push_to_hub(\"[...repo_name...]\", token=True)\n","tokenizer = AutoTokenizer.from_pretrained(\"./custom_LLM_final\")\n","tokenizer.push_to_hub(\"[...repo_name...]\", token=True)\n","```\n","    - `repo_name` 부분에 자신이 만든 모델 repo 이름을 붙여넣는다.\n","    - 만약 colab에서 모델 업로드가 안된다면, 구글 드라이브에서 직접 다운 받아서 repo에 올려주는 방법이 있다.\n","\n","  \n","2. ✔️Huggingface repo에 업로드 된 모델 예시 이미지  \n","<img src = \"https://drive.google.com/uc?id=1yiQv-SeN78H378oTO4UvzvEm4dG3mD0K\"\n","height = 512>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TtWH4V9nVfYl"},"outputs":[],"source":["###########################################\n","# 9-1. 허깅페이스 로그인\n","\n","# WRITE token\n","!huggingface-cli login --token [...write_token...] # your code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S76GCggGzMD5"},"outputs":[],"source":["###########################################\n","# 9-2. 모델 업로드 (~10분)\n","## 먼저, 허깅페이스에 모델 repo를 만든 후 코드를 실행해야 합니다!\n","\n","model = AutoModelForCausalLM.from_pretrained(final_save_folder)\n","model.push_to_hub([...repo_name...], token=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nT3MeKo02vCd"},"outputs":[],"source":["###########################################\n","# 9-3. Tokenizer 업로드\n","\n","tokenizer = AutoTokenizer.from_pretrained(final_save_folder)\n","tokenizer.push_to_hub([...repo_name...], token=True)"]},{"cell_type":"markdown","metadata":{"id":"28oGxX_x3EKS"},"source":["-------\n","# 🤗 Huggingface LLM 리더보드 실습\n","> 🎉여기까지 여러분만의 custom LLM을 만들고 huggingface에 직접 올리는 실습 진행했습니다!!🎉  \n","> 💡자신이 만든 모델을 LLM 리더보드에 올려서 성능평가를 진행하고, 분석을 같이 해봅시다!\n","\n","- [En-LLM 리더보드](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)  \n","- [Ko-LLM 리더보드](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)   \n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1ajjaAkbzRna7Udjf0WAH406jMULnYIY1","timestamp":1700589110156}]},"kernelspec":{"display_name":"Python 3.11.4 ('llama2')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.4"},"vscode":{"interpreter":{"hash":"650f3506fd3966bb017cc36532c1b7da23d4f3599ff501eaa18c43c672ed7c53"}}},"nbformat":4,"nbformat_minor":0}
