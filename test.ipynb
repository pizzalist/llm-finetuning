{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cache_dir = '/home/noah/workspace/dl-study/nlp_study/llama2/cache'\n",
    "\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "    \n",
    "os.environ['HF_HOME'] = cache_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "data_name = 'heegyu/open-korean-instructions'\n",
    "fine_tuning_model_name = f'{model_name}-finetuned-open-korean-instructions'\n",
    "\n",
    "device_map = 'auto'\n",
    "# auto :라이브러리에게 사용 가능한 모든 디바이스를 자동으로 탐색하고 모델의 서로 다른 부분을 이 디바이스들에 자동으로 분배하도록 요청\n",
    "auth_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA의 하이퍼파라미터를 설정 \n",
    "# 알파값을 16으로 설정하여 스케일링\n",
    "# r은 64로 설정\n",
    "# 입력 임베딩 사이즈 64랭크까지 압축\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype='float16',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mletgoofthepizza\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/noah/workspace/dl-study/nlp_study/llama2/wandb/run-20240219_152434-fle6h4nx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/letgoofthepizza/Llama-2-7b-hf-finetuned-open-korean-instructions/runs/fle6h4nx' target=\"_blank\">prosperous-rabbit-4</a></strong> to <a href='https://wandb.ai/letgoofthepizza/Llama-2-7b-hf-finetuned-open-korean-instructions' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/letgoofthepizza/Llama-2-7b-hf-finetuned-open-korean-instructions' target=\"_blank\">https://wandb.ai/letgoofthepizza/Llama-2-7b-hf-finetuned-open-korean-instructions</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/letgoofthepizza/Llama-2-7b-hf-finetuned-open-korean-instructions/runs/fle6h4nx' target=\"_blank\">https://wandb.ai/letgoofthepizza/Llama-2-7b-hf-finetuned-open-korean-instructions/runs/fle6h4nx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/letgoofthepizza/Llama-2-7b-hf-finetuned-open-korean-instructions/runs/fle6h4nx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f77ca08f110>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=fine_tuning_model_name.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'text'],\n",
      "    num_rows: 37516\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(data_name, split='train[:10%]')\n",
    "print((dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'OIG-smallchip2-ko', 'text': '<usr> 저는 발목이 삔 상태이고 더 빨리 낫도록 돕기 위해 제가 무엇을 할 수 있는지 알아야 합니다.\\n<bot> 붓기와 염증을 줄이는 데 도움이 되는 얼음 요법을 시도해 볼 수 있습니다. 또한 탄력 붕대나 압축 랩을 사용하여 발목을 추가로 지지할 수 있습니다. 제대로 치료할 수 있도록 가능한 한 멀리 떨어져 있는 것도 중요합니다.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3844c9b08bb04eddb189be72b9502276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                  quantization_config=bnb_config, # 양자화 설정\n",
    "                                                  use_cache=False, # 모델이 출력을 캐시할지 여부\n",
    "                                                  token=auth_token)\n",
    "base_model.config.pretraining_tp = 1\n",
    "base_model.gradient_checkpointing_enable()\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "peft_model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=auth_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# print(len(tokenizer))\n",
    "model_name = 'beomi/llama-2-ko-7b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=auth_token)\n",
    "print(len(tokenizer.eos_token))\n",
    "print(len(tokenizer.bos_token))\n",
    "print(len(tokenizer.pad_token))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=fine_tuning_model_name,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim='paged_adamw_32bit',\n",
    "    logging_steps=5,\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type='cosine',\n",
    "    disable_tqdm=True,\n",
    "    report_to='wandb',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=min(tokenizer.model_max_length, 2048),\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0731, 'learning_rate': 1.6393442622950818e-05, 'epoch': 0.01}\n",
      "{'loss': 1.0793, 'learning_rate': 3.2786885245901635e-05, 'epoch': 0.01}\n",
      "{'loss': 1.0466, 'learning_rate': 4.918032786885246e-05, 'epoch': 0.02}\n",
      "{'loss': 1.0135, 'learning_rate': 6.557377049180327e-05, 'epoch': 0.03}\n",
      "{'loss': 1.0082, 'learning_rate': 8.19672131147541e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9698, 'learning_rate': 9.836065573770493e-05, 'epoch': 0.04}\n",
      "{'loss': 0.9428, 'learning_rate': 0.00011475409836065574, 'epoch': 0.05}\n",
      "{'loss': 0.9418, 'learning_rate': 0.00013114754098360654, 'epoch': 0.06}\n",
      "{'loss': 0.9139, 'learning_rate': 0.00014754098360655738, 'epoch': 0.07}\n",
      "{'loss': 0.8944, 'learning_rate': 0.0001639344262295082, 'epoch': 0.07}\n",
      "{'loss': 0.8798, 'learning_rate': 0.00018032786885245904, 'epoch': 0.08}\n",
      "{'loss': 0.8416, 'learning_rate': 0.00019672131147540985, 'epoch': 0.09}\n",
      "{'loss': 0.8476, 'learning_rate': 0.00019999790210013988, 'epoch': 0.1}\n",
      "{'loss': 0.8153, 'learning_rate': 0.0001999893795328188, 'epoch': 0.1}\n",
      "{'loss': 0.8132, 'learning_rate': 0.00019997430173759875, 'epoch': 0.11}\n",
      "{'loss': 0.8248, 'learning_rate': 0.00019995266970296855, 'epoch': 0.12}\n",
      "{'loss': 0.8025, 'learning_rate': 0.00019992448484710797, 'epoch': 0.13}\n",
      "{'loss': 0.8375, 'learning_rate': 0.00019988974901779483, 'epoch': 0.13}\n",
      "{'loss': 0.8187, 'learning_rate': 0.0001998484644922837, 'epoch': 0.14}\n",
      "{'loss': 0.7853, 'learning_rate': 0.00019980063397715683, 'epoch': 0.15}\n",
      "{'loss': 0.8066, 'learning_rate': 0.00019974626060814647, 'epoch': 0.16}\n",
      "{'loss': 0.7926, 'learning_rate': 0.00019968534794992949, 'epoch': 0.16}\n",
      "{'loss': 0.7777, 'learning_rate': 0.00019961789999589356, 'epoch': 0.17}\n",
      "{'loss': 0.8031, 'learning_rate': 0.0001995439211678754, 'epoch': 0.18}\n",
      "{'loss': 0.7808, 'learning_rate': 0.00019946341631587087, 'epoch': 0.19}\n",
      "{'loss': 0.7635, 'learning_rate': 0.00019937639071771702, 'epoch': 0.19}\n",
      "{'loss': 0.7719, 'learning_rate': 0.0001992828500787461, 'epoch': 0.2}\n",
      "{'loss': 0.7709, 'learning_rate': 0.00019918280053141143, 'epoch': 0.21}\n",
      "{'loss': 0.779, 'learning_rate': 0.0001990762486348855, 'epoch': 0.22}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:331\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trl_activate_neftune(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m--> 331\u001b[0m output \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtrain(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    333\u001b[0m \u001b[39m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneftune_noise_alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/trainer.py:1874\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m   1869\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1871\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1874\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39;49misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1878\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "GatedRepoError",
     "evalue": "401 Client Error. (Request ID: Root=1-65d30b97-7de783db232005b877a13825;2a53640e-01a4-4764-bd99-609606d431d3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nRepo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    287\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/trainer.py:2886\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2883\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped\u001b[39m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2885\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[0;32m-> 2886\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save(output_dir)\n\u001b[1;32m   2888\u001b[0m \u001b[39m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2889\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/trainer.py:2958\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2956\u001b[0m             torch\u001b[39m.\u001b[39msave(state_dict, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   2957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2958\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49msave_pretrained(\n\u001b[1;32m   2959\u001b[0m         output_dir, state_dict\u001b[39m=\u001b[39;49mstate_dict, safe_serialization\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49msave_safetensors\n\u001b[1;32m   2960\u001b[0m     )\n\u001b[1;32m   2962\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2963\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/peft/peft_model.py:216\u001b[0m, in \u001b[0;36mPeftModel.save_pretrained\u001b[0;34m(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m peft_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpeft_config[adapter_name]\n\u001b[1;32m    215\u001b[0m \u001b[39m# save only the trainable weights\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m output_state_dict \u001b[39m=\u001b[39m get_peft_model_state_dict(\n\u001b[1;32m    217\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    218\u001b[0m     state_dict\u001b[39m=\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstate_dict\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    219\u001b[0m     adapter_name\u001b[39m=\u001b[39;49madapter_name,\n\u001b[1;32m    220\u001b[0m     save_embedding_layers\u001b[39m=\u001b[39;49msave_embedding_layers,\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    222\u001b[0m output_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(save_directory, adapter_name) \u001b[39mif\u001b[39;00m adapter_name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m save_directory\n\u001b[1;32m    223\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/peft/utils/save_and_load.py:146\u001b[0m, in \u001b[0;36mget_peft_model_state_dict\u001b[0;34m(model, state_dict, adapter_name, unwrap_compiled, save_embedding_layers)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m model_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         has_remote_config \u001b[39m=\u001b[39m file_exists(model_id, \u001b[39m\"\u001b[39;49m\u001b[39mconfig.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    147\u001b[0m     \u001b[39mexcept\u001b[39;00m (HFValidationError, EntryNotFoundError):\n\u001b[1;32m    148\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    149\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not find a config file in \u001b[39m\u001b[39m{\u001b[39;00mmodel_id\u001b[39m}\u001b[39;00m\u001b[39m - will assume that the vocabulary was not modified.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2386\u001b[0m, in \u001b[0;36mHfApi.file_exists\u001b[0;34m(self, repo_id, filename, repo_type, revision, token)\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2385\u001b[0m         token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken\n\u001b[0;32m-> 2386\u001b[0m     get_hf_file_metadata(url, token\u001b[39m=\u001b[39;49mtoken)\n\u001b[1;32m   2387\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2388\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError:  \u001b[39m# raise specifically on gated repo\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1628\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mAccept-Encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1632\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1633\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1634\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1635\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1636\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1637\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1638\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1639\u001b[0m )\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1642\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    386\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    387\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    388\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    389\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    392\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m    408\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:302\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGatedRepo\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    299\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    300\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot access gated repo for url \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m     )\n\u001b[0;32m--> 302\u001b[0m     \u001b[39mraise\u001b[39;00m GatedRepoError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39melif\u001b[39;00m error_code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRepoNotFound\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    305\u001b[0m     response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m401\u001b[39m\n\u001b[1;32m    306\u001b[0m     \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mrequest \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[39m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m make sure you are authenticated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-65d30b97-7de783db232005b877a13825;2a53640e-01a4-4764-bd99-609606d431d3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nRepo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it."
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('llama2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "650f3506fd3966bb017cc36532c1b7da23d4f3599ff501eaa18c43c672ed7c53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
