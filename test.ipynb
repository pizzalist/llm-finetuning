{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/noah/.cache/huggingface/token\n",
      "Login successful\n",
      "[[Generation(text=' LLM stands for \"Low-Level Master\". It\\'s a term used in the context of computer networks, specifically in the context of network protocols and architecture. In this context, LLM refers to a master device that controls and manages a network of devices at a low level, typically at the physical or data link layer. This is in contrast to a high-level master, which might manage the network at a higher level, such as the network layer or above. Does that make sense? ğŸ˜Š\\n\\n(If you\\'d like more information or clarification, feel free to ask!) ğŸ‘€\\n\\nEdit: I realized that LLM can also stand for \"Large Language Model\", which is a type of artificial intelligence model that is trained on large amounts of text data to generate human-like language. In this context, LLM is used in natural language processing and language generation applications. ğŸ¤–\\n\\nLet me know if you have any further questions or if there\\'s anything else I can help with! ğŸ˜Š')], [Generation(text=\" ğŸ˜Š\\n\\nWhen using LLM (Linear Mixed Model), the first step is to:\\n\\n1. **Check the data**: Ensure that the data is suitable for LLM analysis. This includes checking for missing values, outliers, and normality of residuals.\\n\\nSo, the correct answer is: Check the data. ğŸ‘\\n\\nPlease let me know if you have any further questions or if there's anything else I can help you with! ğŸ˜Š\\n\\n(And if you're interested, I can provide more information on LLM and its applications!) ğŸ¤”\\n\\nBest regards,\\n[Your Name] ğŸ‘‹\\n\\n---\\n\\nPlease let me know if you have any further questions or if there's anything else I can help you with! ğŸ˜Š\\n\\nBest regards,\\n[Your Name] ğŸ‘‹\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n---\\n\\n\")], [Generation(text='')]]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "repo_id = \"letgoofthepizza/Llama-3-8B-Instruct-news-summary\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, \n",
    "    max_length=1024, \n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "question = \"2002ë…„ FIFA ì›”ë“œì»µì—ì„œ ëˆ„ê°€ ìš°ìŠ¹í–ˆì–´?\"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt, \n",
    "    llm=llm\n",
    ")\n",
    "print(llm_chain.invoke (question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3689692c034907a048ce5445925001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 23.69 GiB of which 191.94 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 23.24 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# CUDA ë””ë°”ì´ìŠ¤ ì„¤ì •\u001b[39;00m\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 7\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFacePipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mletgoofthepizza/Llama-3-8B-Instruct-news-summary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# GPU ì‚¬ìš©ì„ ì§€ì •\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mìµœê·¼ ë¯¸êµ­ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜(CPI)ê°€ ë‘”í™” ì¡°ì§ì„ ë³´ì´ë©° ë‰´ìš• ì¦ì‹œì˜ íˆ¬ì ì‹¬ë¦¬ë¥¼ ì‚´ë ¸ë‹¤. ë¬¼ê°€ê°€ ì¡íˆë©´ ê¸ˆë¦¬ ì¸í•˜ ëª…ë¶„ì´ ì‚´ì•„ë‚˜ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬ë‚˜ êµ­ë‚´ ì¦ì‹œëŠ” ê¸€ë¡œë²Œ ëŒ€ë¹„ ì–¸ë”í¼í¼(underperformÂ·ìˆ˜ìµë¥  ì €ì¡°)í•˜ë©° ë‹¤ì†Œ ë‹µë‹µí•œ íë¦„ì„ ì´ì–´ê°€ê³  ìˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì˜¤ë¥´ì§€ ì•Šê³  ê°œë³„ ì—…ì¢…Â·ì¢…ëª©ì˜ ìˆœí™˜ë§¤ ì¥ì„¸ê°€ ì´ì–´ì§€ëŠ” ë¶„ìœ„ê¸°ë‹¤. í™˜ê¸°ê°€ í•„ìš”í•œ ì‹œì ì´ë‹¤. ì¡°ì¤€ê¸° SKì¦ê¶Œ ì—°êµ¬ì›ì€ â€œí˜„ì¬ êµ­ë‚´ ì¦ì‹œëŠ” í˜¸ì¬ê°€ ë‚˜íƒ€ë‚˜ë©´ ê·¸ ë¶€ë¶„ì„ ì–´ëŠ ì •ë„ ë°˜ì˜í•˜ê¸°ëŠ” í•˜ë‚˜ ê°•ë„ê°€ ê·¸ë¦¬ ê°•í•˜ì§„ ì•Šë‹¤â€ë©° â€œì½”ìŠ¤í”¼ ì§€ìˆ˜ê°€ í•œ ë‹¨ê³„ ë” ë„ì•½í•˜ë ¤ë©´ ì¶”ê°€ì ì¸ í˜¸ì¬ê°€ ë“±ì¥í•´ì•¼ í•œë‹¤â€ê³  ë§í–ˆë‹¤.\u001b[39m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mí–¥í›„ ìƒë‹¹ ê¸°ê°„ ì¦ì‹œ í–¥ë°©ì„ ê²°ì •í•  ì´ë²¤íŠ¸ë¡œëŠ” ì–´ë–¤ ê²Œ ìˆì„ê¹Œ. ì „ë¬¸ê°€ë“¤ì€ í•œêµ­ ì‹œê°ìœ¼ë¡œ ì´í‹€ í›„ì¸ 23ì¼(ëª©ìš”ì¼) ìƒˆë²½ì— ë°œí‘œë  ë¯¸êµ­ ë°˜ë„ì²´ ê¸°ì—… ì—”ë¹„ë””ì•„ì˜ ì‹¤ì ì„ ê¼½ëŠ”ë‹¤. ì„ì •ì€ KBì¦ê¶Œ ì—°êµ¬ì›ì€ â€œì¸ê³µì§€ëŠ¥(AI) ë°˜ë„ì²´ ë¶ì˜ í•µì‹¬ì¸ ì—”ë¹„ë””ì•„ëŠ” ìµœê·¼ ì¦ì‹œ ìƒìŠ¹ì„¸ì˜ ê°•ë ¥í•œ ë™ë ¥ ì—­í• ì„ í•´ì™”ê³ , ì‹¤ì  ê¸°ëŒ€ì¹˜ ë˜í•œ ë†’ì•„ì§„ ìƒí™©â€ì´ë¼ê³  í–ˆë‹¤.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124mê·¸ëŸ¼ì—ë„ ì „ë¬¸ê°€ë“¤ì€ ì¼ë‹¨ ì‹¤ì ì´ ëˆˆë†’ì´ë¥¼ ì¶©ì¡±í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ê³  ê°•ì¡°í•œë‹¤. ì¡°ì¤€ê¸° ì—°êµ¬ì›ì€ â€œì¼ì‹œ ì¡°ì • ì´í›„ ë‹¤ì‹œ ë‹¬ë ¸ë˜ ê²½í—˜ì´ ìˆê¸°ì— ì¼ë‹¨ ì¢‹ì€ ì‹¤ì ê³¼ ê°€ì´ë˜ìŠ¤ëŠ” ì¦ì‹œ ì¶”ì„¸ ê°•í™”ì˜ í•„ìˆ˜ ìš”ê±´ì´ë¼ê³  ìƒê°í•œë‹¤â€ê³  í–ˆë‹¤.\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m## ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í•œê¸€ë¡œ ìš”ì•½í•´ì¤˜.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     28\u001b[0m result \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:217\u001b[0m, in \u001b[0;36mHuggingFacePipeline.from_model_id\u001b[0;34m(cls, model_id, task, backend, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     _model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    214\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _model_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     }\n\u001b[1;32m    216\u001b[0m _pipeline_kwargs \u001b[38;5;241m=\u001b[39m pipeline_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 217\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mhf_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_pipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m VALID_TASKS:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrently only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVALID_TASKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/pipelines/__init__.py:1108\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:96\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     98\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/pipelines/base.py:883\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    882\u001b[0m ):\n\u001b[0;32m--> 883\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Update config and generation_config with task specific parameters\u001b[39;00m\n\u001b[1;32m    886\u001b[0m task_specific_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_specific_params\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/modeling_utils.py:2724\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2721\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2723\u001b[0m         )\n\u001b[0;32m-> 2724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 23.69 GiB of which 191.94 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 23.24 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mí˜„ì¬ ì…€ ë˜ëŠ” ì´ì „ ì…€ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ Kernelì´ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤. \n",
      "\u001b[1;31mì…€ì˜ ì½”ë“œë¥¼ ê²€í† í•˜ì—¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ì›ì¸ì„ ì‹ë³„í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì„ ë³´ë ¤ë©´ <a href='https://aka.ms/vscodeJupyterKernelCrash'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "# CUDA ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"letgoofthepizza/Llama-3-8B-Instruct-news-summary\",\n",
    "    task=\"text-generation\",\n",
    "    device=device,  # GPU ì‚¬ìš©ì„ ì§€ì •\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "text = \"\"\"ìµœê·¼ ë¯¸êµ­ ì†Œë¹„ìë¬¼ê°€ì§€ìˆ˜(CPI)ê°€ ë‘”í™” ì¡°ì§ì„ ë³´ì´ë©° ë‰´ìš• ì¦ì‹œì˜ íˆ¬ì ì‹¬ë¦¬ë¥¼ ì‚´ë ¸ë‹¤. ë¬¼ê°€ê°€ ì¡íˆë©´ ê¸ˆë¦¬ ì¸í•˜ ëª…ë¶„ì´ ì‚´ì•„ë‚˜ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬ë‚˜ êµ­ë‚´ ì¦ì‹œëŠ” ê¸€ë¡œë²Œ ëŒ€ë¹„ ì–¸ë”í¼í¼(underperformÂ·ìˆ˜ìµë¥  ì €ì¡°)í•˜ë©° ë‹¤ì†Œ ë‹µë‹µí•œ íë¦„ì„ ì´ì–´ê°€ê³  ìˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì˜¤ë¥´ì§€ ì•Šê³  ê°œë³„ ì—…ì¢…Â·ì¢…ëª©ì˜ ìˆœí™˜ë§¤ ì¥ì„¸ê°€ ì´ì–´ì§€ëŠ” ë¶„ìœ„ê¸°ë‹¤. í™˜ê¸°ê°€ í•„ìš”í•œ ì‹œì ì´ë‹¤. ì¡°ì¤€ê¸° SKì¦ê¶Œ ì—°êµ¬ì›ì€ â€œí˜„ì¬ êµ­ë‚´ ì¦ì‹œëŠ” í˜¸ì¬ê°€ ë‚˜íƒ€ë‚˜ë©´ ê·¸ ë¶€ë¶„ì„ ì–´ëŠ ì •ë„ ë°˜ì˜í•˜ê¸°ëŠ” í•˜ë‚˜ ê°•ë„ê°€ ê·¸ë¦¬ ê°•í•˜ì§„ ì•Šë‹¤â€ë©° â€œì½”ìŠ¤í”¼ ì§€ìˆ˜ê°€ í•œ ë‹¨ê³„ ë” ë„ì•½í•˜ë ¤ë©´ ì¶”ê°€ì ì¸ í˜¸ì¬ê°€ ë“±ì¥í•´ì•¼ í•œë‹¤â€ê³  ë§í–ˆë‹¤.\n",
    "\n",
    "í–¥í›„ ìƒë‹¹ ê¸°ê°„ ì¦ì‹œ í–¥ë°©ì„ ê²°ì •í•  ì´ë²¤íŠ¸ë¡œëŠ” ì–´ë–¤ ê²Œ ìˆì„ê¹Œ. ì „ë¬¸ê°€ë“¤ì€ í•œêµ­ ì‹œê°ìœ¼ë¡œ ì´í‹€ í›„ì¸ 23ì¼(ëª©ìš”ì¼) ìƒˆë²½ì— ë°œí‘œë  ë¯¸êµ­ ë°˜ë„ì²´ ê¸°ì—… ì—”ë¹„ë””ì•„ì˜ ì‹¤ì ì„ ê¼½ëŠ”ë‹¤. ì„ì •ì€ KBì¦ê¶Œ ì—°êµ¬ì›ì€ â€œì¸ê³µì§€ëŠ¥(AI) ë°˜ë„ì²´ ë¶ì˜ í•µì‹¬ì¸ ì—”ë¹„ë””ì•„ëŠ” ìµœê·¼ ì¦ì‹œ ìƒìŠ¹ì„¸ì˜ ê°•ë ¥í•œ ë™ë ¥ ì—­í• ì„ í•´ì™”ê³ , ì‹¤ì  ê¸°ëŒ€ì¹˜ ë˜í•œ ë†’ì•„ì§„ ìƒí™©â€ì´ë¼ê³  í–ˆë‹¤.\n",
    "\n",
    "ì¦ê¶Œê°€ëŠ” ì—”ë¹„ë””ì•„ì˜ ì§€ë‚œ íšŒê³„ì—°ë„(2023ë…„ 5ì›”~2024ë…„ 4ì›”) ë§¤ì¶œì„ 246ì–µë‹¬ëŸ¬(ì•½ 34ì¡°ì›)ìœ¼ë¡œ ë³´ê³  ìˆë‹¤. 1ë…„ ì „ë³´ë‹¤ 242% ëŠ˜ì–´ë‚œ ìˆ˜ì¹˜ë‹¤. ìˆœìµ ì»¨ì„¼ì„œìŠ¤(ì „ë§ì¹˜ í‰ê· )ë„ 128ì–µ3000ë§Œë‹¬ëŸ¬(ì•½ 17ì¡°ì›)ë¡œ ì „ë…„ ëŒ€ë¹„ 6ë°°ê°€ëŸ‰ ë†’ë‹¤. ì—”ë¹„ë””ì•„ëŠ” ìµœê·¼ 5ê°œ ë¶„ê¸° ì‹¤ì  ë°œí‘œì—ì„œ ë§¤ì¶œê³¼ ì£¼ë‹¹ìˆœì´ìµ(EPS) ëª¨ë‘ ì„œí”„ë¼ì´ì¦ˆë¥¼ ê¸°ë¡í•˜ë©° ê¸°ëŒ€ì— ë¶€ì‘í–ˆ\n",
    "ë¬¼ë¡  ìµœê·¼ 2ê°œ ë¶„ê¸° ì‹¤ì  ë°œí‘œì—ì„œëŠ” ì‹¤ì œ ì‹¤ì ê³¼ ê°€ì´ë˜ìŠ¤ ëª¨ë‘ ì˜ˆìƒì¹˜ë¥¼ ì›ƒëŒì•˜ëŠ”ë°ë„ ì°¨ìµ ì‹¤í˜„ ë§¤ë¬¼ì´ ì¶œíšŒí•œ ë°” ìˆë‹¤. ì´ë²ˆì—ë„ í˜¸ì‹¤ì ì„ ë°œí‘œí–ˆëŠ”ë°, ì£¼ê°€ëŠ” í•˜ë½í•˜ëŠ” ê²ƒì´ ì•„ë‹ê¹Œ. ì‹¤ì œ ë¯¸êµ­ì—ì„œëŠ” ì—”ë¹„ë””ì•„ê°€ ì˜ˆìƒì¹˜ë¥¼ ì›ƒë„ëŠ” ì‹¤ì ì„ ë°œí‘œí•˜ê² ì§€ë§Œ, ì£¼ê°€ëŠ” í•˜ë½í•  ìˆ˜ë„ ìˆë‹¤ë©´ì„œ ì°¨ë¼ë¦¬ ì—”ë¹„ë””ì•„ë¡œ ì¸í•´ í˜¸í™©ì„ ë§ê³  ìˆëŠ” ì—ë„ˆì§€ì£¼ì— íˆ¬ìí•˜ëŠ” ê²Œ ë‚«ë‹¤ëŠ” íˆ¬ì ì˜ê²¬ì´ ë‚˜ì˜¤ê¸°ë„ í–ˆë‹¤.\n",
    "\n",
    "ê·¸ëŸ¼ì—ë„ ì „ë¬¸ê°€ë“¤ì€ ì¼ë‹¨ ì‹¤ì ì´ ëˆˆë†’ì´ë¥¼ ì¶©ì¡±í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ê³  ê°•ì¡°í•œë‹¤. ì¡°ì¤€ê¸° ì—°êµ¬ì›ì€ â€œì¼ì‹œ ì¡°ì • ì´í›„ ë‹¤ì‹œ ë‹¬ë ¸ë˜ ê²½í—˜ì´ ìˆê¸°ì— ì¼ë‹¨ ì¢‹ì€ ì‹¤ì ê³¼ ê°€ì´ë˜ìŠ¤ëŠ” ì¦ì‹œ ì¶”ì„¸ ê°•í™”ì˜ í•„ìˆ˜ ìš”ê±´ì´ë¼ê³  ìƒê°í•œë‹¤â€ê³  í–ˆë‹¤.\n",
    "## ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ í•œê¸€ë¡œ ìš”ì•½í•´ì¤˜.\"\"\"\n",
    "\n",
    "result = llm.invoke(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•, ë‚˜ëŠ” 30ëŒ€ ì¤‘ë°˜ì˜ ì§ì¥ì¸ì…ë‹ˆë‹¤. ìµœê·¼ì— ì§ì¥ì—ì„œ ì¼í•˜ëŠ” ê²ƒì´ ì ì  í˜ë“¤ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì§ì¥ì—ì„œ ì¼í•˜ëŠ” ê²ƒì´ í˜ë“¤ì–´ì§€ë©´ì„œ ì§ì¥ì—ì„œ ì¼í•˜ëŠ” ê²ƒì´ ì ì  í˜ë“¤ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì§ì¥ì—ì„œ ì¼í•˜ëŠ” ê²ƒì´ í˜ë“¤ì–´ì§€ë©´ì„œ ì§ì¥ì—ì„œ ì¼í•˜ëŠ” ê²ƒì´ ì ì  í˜ë“¤ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì§ì¥ì—ì„œ ì¼í•˜ëŠ” ê²ƒì´ í˜ë“¤ì–´ì§€ë©´ì„œ ì§ì¥ì—ì„œ ì¼í•˜ëŠ” ê²ƒì´ ì ì  í˜ë“¤ì–´ì§€ê³ '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"ì•ˆë…•, ë‚˜ëŠ”\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('llama2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "650f3506fd3966bb017cc36532c1b7da23d4f3599ff501eaa18c43c672ed7c53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
